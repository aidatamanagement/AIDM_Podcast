<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
  xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">

  <channel>
    <title>The Unstructured Engineer</title>
    <link>https://aidatamanagement.com/</link>
    <language>en-us</language>
    <description>The Unstructured Engineer is an AI-created podcast that breaks down complex topics in AI data management, governance, implementation, and enterprise AI strategy—making them accessible, actionable, and aligned with real business outcomes.
      
Hosted by Nicole Williams, an advanced AI avatar designed to deliver clear, engaging, executive-level insights, the show turns written thought leadership into a dynamic listening experience. Episodes are concise, human-sounding, and built entirely on evidence-based AIDM content—not opinions or hype.
      
Whether you're building AI readiness, improving data foundations, or exploring generative AI applications, The Unstructured Engineer helps leaders stay ahead by focusing on foundation before innovation, augmented intelligence, and measurable ROI.
      
Created by AI. Delivered by AI. Engineered for humans.</description>

    <!-- Set this to the final public URL where THIS RSS XML is hosted -->
    <atom:link href="https://raw.githubusercontent.com/aidatamanagement/AIDM_Podcast/main/Supabase_Podcast.xml" rel="self" type="application/rss+xml"/>

    <!-- Recommended metadata -->
    <lastBuildDate>Wed, 03 Dec 2025 10:15:00 -0500</lastBuildDate>
    <itunes:type>episodic</itunes:type>

    <!-- Spotify reads iTunes podcast tags -->
    <itunes:author>Nicole Williams - AI Avatar</itunes:author>
    <itunes:summary>A podcast powered by Supabase-hosted audio files.</itunes:summary>
    <itunes:explicit>false</itunes:explicit>

    <itunes:owner>
      <itunes:name>Shubakar Poda</itunes:name>
      <itunes:email>hello@aidatamanagement.com</itunes:email>
    </itunes:owner>

    <itunes:category text="Technology"/>

    <!-- Prefer hosting your own cover art long-term, but this URL is syntactically fine -->
    <itunes:image href="https://is1-ssl.mzstatic.com/image/thumb/Podcasts221/v4/ff/6d/e4/ff6de4b5-7eda-aa9a-bb9e-aedcfe6d72d8/mza_7740481562964404675.jpg/1200x1200bf-60.jpg"/>

    <!-- Optional nice-to-haves -->
    <itunes:subtitle>Supabase-hosted podcast feed test</itunes:subtitle>
    <itunes:keywords>supabase, podcast, testing, aidm</itunes:keywords>



  
    <item>
      <title><![CDATA[The AI champion playbook: Building internal advocates in 30 days]]></title>
      <description><![CDATA[Many organizations find themselves struggling with widespread AI adoption. Traditional top-down mandates often fall short, failing to foster genuine engagement and practical application. In fact, research indicates that nearly a third of AI usage happens without management knowledge, highlighting a significant disconnect and the organic emergence of what some call "shadow AI." This phenomenon underscores the need for a more inclusive, grassroots approach to integrating AI into daily operations.

At AI Data Management, we advocate for foundation before innovation—a principle that extends to how AI capabilities are diffused throughout an enterprise. Rather than simply forcing new tools, the most effective strategy involves cultivating internal AI champions. These early adopters become trusted peers, demystifying AI, demonstrating its value, and driving organic enthusiasm throughout the organization. This approach ensures sustainable and impactful AI transformation.

The AI champion model posits that a small group of highly motivated individuals, equipped with knowledge and practical skills, can become powerful internal trainers and subject matter experts. They serve as the go-to AI resources, mentoring peers and answering day-to-day questions. This approach harnesses intrinsic motivation, fostering an environment where innovation spreads naturally rather than being imposed.

Successful companies are those that involve employees from all levels in AI adoption. This inclusive strategy recognizes that while leadership advocacy is important—making organizations twenty-two percent more likely to see widespread adoption—the true transformation happens when individuals at the operational level embrace and integrate these tools into their workflows. With sixty-seven percent of employees personally pushing for AI adoption, there's a clear appetite for engagement.

Here's a thirty-day playbook to identify, train, and empower these critical internal advocates.

**Week One: Identification – Finding Your Advocates**

The first step is to identify individuals who are genuinely interested and well-positioned to become champions. The key here is to find volunteers, not what some call "volun-tolds." The most motivated advocates will actively raise their hands when asked.

Look for natural problem-solvers. Go beyond just tech enthusiasts. The ideal champion is someone who consistently seeks efficient ways to solve problems within their department, regardless of their current technical proficiency. Prioritize diverse representation. Ensure champions come from various departments and roles. This diversity is crucial for identifying a broad range of use cases and ensuring relevance across the organization.

**Week Two: Intensive Training – Equipping for Impact**

Once identified, champions require hands-on, practical training to build their confidence and competence. This phase is about empowerment and immediate application. Provide direct access and guided sessions with AI tools relevant to your organization. This should go beyond theoretical understanding to practical usage.

Challenge champions to develop a small, custom AI solution directly applicable to a pain point or efficiency gap within their own department. This practical application solidifies their learning. Guide champions in spotting opportunities where AI can deliver immediate, measurable benefits. These quick wins are vital for building early momentum and demonstrating value. This phase aligns with a strategy of continuous upskilling, where teams are trained on AI tools because it's a critical skill for their career development and marketability.

**Week Three: Pilot Success – Demonstrating Value**

With training complete, champions move into applying their new skills in their teams, proving the value of AI through real-world implementation. They introduce their custom solutions or apply AI tools to specific tasks within their department, acting as pioneers.

Crucially, champions must track the tangible benefits—things like time saved, errors reduced, or productivity increased. Quantifiable results are powerful motivators for wider adoption. Encourage champions to gather feedback from their colleagues and use it to refine their solutions or identify further opportunities. This iterative process builds better tools and stronger advocacy.

**Week Four: Peer Teaching – Scaling Knowledge**

The final week transforms champions into official internal educators, leveraging their credibility and practical experience to scale adoption. They conduct workshops and informal training sessions for their colleagues, using their personal success stories as compelling examples.

Nothing convinces like seeing a peer benefit directly from AI. Champions share their documented quick wins and customized solutions, making AI relatable and actionable. They become the first line of support, patiently answering questions and troubleshooting minor issues, building trust and reducing anxiety around new technologies.

To illustrate, a leading energy company successfully implemented an AI champion model, achieving an impressive ninety percent AI adoption rate across several departments within just six months. By identifying motivated employees in operational roles and empowering them with tailored training to solve specific challenges—such as optimizing maintenance schedules or predicting equipment failures—the company witnessed organic enthusiasm spread. These champions not only streamlined critical processes but also cultivated a culture of innovation, demonstrating that targeted, peer-led initiatives can significantly accelerate enterprise-wide AI transformation.

Building an AI-powered workforce isn't merely a technological upgrade; it's a cultural shift. By empowering internal AI champions, organizations move beyond mandated adoption to foster organic enthusiasm and practical integration. This thirty-day playbook provides a clear, actionable framework to create a network of advocates who will not only drive immediate efficiencies but also lay a solid foundation before innovation for future AI initiatives. The true power of AI unfolds when it's embraced, understood, and championed by the very people who stand to benefit most from its transformative potential.]]></description>
      <content:encoded><![CDATA[Many organizations find themselves struggling with widespread AI adoption. Traditional top-down mandates often fall short, failing to foster genuine engagement and practical application. In fact, research indicates that nearly a third of AI usage happens without management knowledge, highlighting a significant disconnect and the organic emergence of what some call "shadow AI." This phenomenon underscores the need for a more inclusive, grassroots approach to integrating AI into daily operations.

At AI Data Management, we advocate for foundation before innovation—a principle that extends to how AI capabilities are diffused throughout an enterprise. Rather than simply forcing new tools, the most effective strategy involves cultivating internal AI champions. These early adopters become trusted peers, demystifying AI, demonstrating its value, and driving organic enthusiasm throughout the organization. This approach ensures sustainable and impactful AI transformation.

The AI champion model posits that a small group of highly motivated individuals, equipped with knowledge and practical skills, can become powerful internal trainers and subject matter experts. They serve as the go-to AI resources, mentoring peers and answering day-to-day questions. This approach harnesses intrinsic motivation, fostering an environment where innovation spreads naturally rather than being imposed.

Successful companies are those that involve employees from all levels in AI adoption. This inclusive strategy recognizes that while leadership advocacy is important—making organizations twenty-two percent more likely to see widespread adoption—the true transformation happens when individuals at the operational level embrace and integrate these tools into their workflows. With sixty-seven percent of employees personally pushing for AI adoption, there's a clear appetite for engagement.

Here's a thirty-day playbook to identify, train, and empower these critical internal advocates.

**Week One: Identification – Finding Your Advocates**

The first step is to identify individuals who are genuinely interested and well-positioned to become champions. The key here is to find volunteers, not what some call "volun-tolds." The most motivated advocates will actively raise their hands when asked.

Look for natural problem-solvers. Go beyond just tech enthusiasts. The ideal champion is someone who consistently seeks efficient ways to solve problems within their department, regardless of their current technical proficiency. Prioritize diverse representation. Ensure champions come from various departments and roles. This diversity is crucial for identifying a broad range of use cases and ensuring relevance across the organization.

**Week Two: Intensive Training – Equipping for Impact**

Once identified, champions require hands-on, practical training to build their confidence and competence. This phase is about empowerment and immediate application. Provide direct access and guided sessions with AI tools relevant to your organization. This should go beyond theoretical understanding to practical usage.

Challenge champions to develop a small, custom AI solution directly applicable to a pain point or efficiency gap within their own department. This practical application solidifies their learning. Guide champions in spotting opportunities where AI can deliver immediate, measurable benefits. These quick wins are vital for building early momentum and demonstrating value. This phase aligns with a strategy of continuous upskilling, where teams are trained on AI tools because it's a critical skill for their career development and marketability.

**Week Three: Pilot Success – Demonstrating Value**

With training complete, champions move into applying their new skills in their teams, proving the value of AI through real-world implementation. They introduce their custom solutions or apply AI tools to specific tasks within their department, acting as pioneers.

Crucially, champions must track the tangible benefits—things like time saved, errors reduced, or productivity increased. Quantifiable results are powerful motivators for wider adoption. Encourage champions to gather feedback from their colleagues and use it to refine their solutions or identify further opportunities. This iterative process builds better tools and stronger advocacy.

**Week Four: Peer Teaching – Scaling Knowledge**

The final week transforms champions into official internal educators, leveraging their credibility and practical experience to scale adoption. They conduct workshops and informal training sessions for their colleagues, using their personal success stories as compelling examples.

Nothing convinces like seeing a peer benefit directly from AI. Champions share their documented quick wins and customized solutions, making AI relatable and actionable. They become the first line of support, patiently answering questions and troubleshooting minor issues, building trust and reducing anxiety around new technologies.

To illustrate, a leading energy company successfully implemented an AI champion model, achieving an impressive ninety percent AI adoption rate across several departments within just six months. By identifying motivated employees in operational roles and empowering them with tailored training to solve specific challenges—such as optimizing maintenance schedules or predicting equipment failures—the company witnessed organic enthusiasm spread. These champions not only streamlined critical processes but also cultivated a culture of innovation, demonstrating that targeted, peer-led initiatives can significantly accelerate enterprise-wide AI transformation.

Building an AI-powered workforce isn't merely a technological upgrade; it's a cultural shift. By empowering internal AI champions, organizations move beyond mandated adoption to foster organic enthusiasm and practical integration. This thirty-day playbook provides a clear, actionable framework to create a network of advocates who will not only drive immediate efficiencies but also lay a solid foundation before innovation for future AI initiatives. The true power of AI unfolds when it's embraced, understood, and championed by the very people who stand to benefit most from its transformative potential.]]></content:encoded>
      <guid isPermaLink="false">TheAIchampionplaybookBuildinginternaladvocatesin30days</guid>
      <pubDate>Thu, 04 Dec 2025 19:46:57 GMT</pubDate>
      <enclosure
        url="https://oimqzyfmglyhljjuboek.supabase.co/storage/v1/object/public/AIDM_Podcast_Audio_Files/TheAIchampionplaybookBuildinginternaladvocatesin30days.mp3"
        length="12345678"
        type="audio/mpeg" />
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:explicit>false</itunes:explicit>
    </item>

  
    <item>
      <title><![CDATA[The hidden costs of AI unreadiness (and how to avoid them)]]></title>
      <description><![CDATA[Imagine pouring half a million dollars into cutting-edge AI tools, only to see zero improvement in efficiency. Worse, productivity actually drops. This isn't a hypothetical disaster; it's a common reality for many organizations. The problem isn't the AI itself. It's a critical gap in strategy: a lack of readiness.

Most companies budget for the obvious costs of AI adoption, but it's the hidden expenditures that truly erode ROI and sabotage transformation efforts. Successfully integrating AI requires much more than just buying technology. It demands a robust data foundation, strategic planning, and a prepared workforce.

Organizations typically account for direct expenses: software licenses, implementation fees, employee training. These look manageable on a balance sheet. But they're just the tip of the iceberg, a fraction of the true investment needed for AI success.

The real financial drain and strategic setback come from the invisible costs of unreadiness. These insidious factors turn promising AI projects into costly failures, impacting budgets, employee morale, and long-term innovation.

First, rework due to poor data quality. AI models are only as good as the data they're trained on. If data is inconsistent, incomplete, or inaccurate, AI outputs will require extensive manual fixing and validation. That immediately negates any efficiency gains AI promises. This often forces costly data readiness assessments just to clean and structure data effectively, *before* an organization can even launch an AI pilot.

Next, employee resistance and shadow systems. When AI tools are introduced without adequate preparation, training, or a clear perceived benefit, employees often resist. This resistance manifests as workarounds, wasted time, and the creation of shadow IT systems—unofficial tools and processes that undermine centralized AI efforts and create new security and data governance challenges.

Paradoxically, AI can sometimes increase, not decrease, employee workload and lead to burnout if not implemented thoughtfully. An Upwork study found that seventy-seven percent of employees felt AI tools actually *increased* their workload. Seventy-one percent reported burnout. When AI initiatives add to employee burden without providing clear benefits or offloading tedious tasks, it leads to frustration, disengagement, and increased employee turnover. That's a substantial loss of institutional knowledge and productivity.

Then there are significant opportunity costs. Failed AI projects don't just waste resources; they delay genuine transformation by eighteen to twenty-four months or more. This means competitors who successfully leverage AI pull ahead in efficiency, innovation, and market share. Many seemingly high-value AI use cases, like productivity improvements, can actually result in a negative ROI when deployed at scale, if an organization doesn't plan out the platform, data readiness, and governance costs.

The cumulative effect of these hidden costs is staggering. A poorly planned or executed AI implementation can end up costing an organization three to four times its original investment. This includes not just the initial outlay, but also the expenses of retooling, re-training, fixing data, managing resistance, and ultimately restarting initiatives from scratch. The real-world consequences of rushing AI integration without thorough planning are evident in these magnified costs.

Achieving a positive return on AI investment hinges on establishing a solid foundation. AI readiness isn't an optional extra; it's a prerequisite for success. The philosophy is clear: foundation before innovation. This means ensuring data is clean, accessible, and governed. Infrastructure must be capable, and the team equipped with the right skills and mindset. Without this readiness, AI becomes a liability, not an asset.

Avoiding these pitfalls requires a systematic and strategic approach. First, conduct comprehensive AI readiness assessments *before* investment. Understand current capabilities, data quality, and organizational culture before committing significant funds to tools. This involves evaluating, aligning, and accelerating AI programs with structured assessments and tailored workshops.

Second, pilot before scaling. Start with small, controlled pilot projects to test AI solutions, validate effectiveness, and identify potential challenges in a low-risk environment. This iterative approach allows for learning and adjustment.

Third, training before rollout. Invest in robust training programs that not only teach employees how to use AI tools, but also explain the 'why' behind the technology. Foster a culture of augmented intelligence where AI supports human capabilities, rather than replacing them.

Consider a non-profit organization that invested one hundred thousand dollars in AI tools without first addressing its chaotic data landscape. The organization quickly found its AI outputs unusable. It was forced to spend an additional two hundred thousand dollars on data cleanup and governance *before* seeing any value. This clearly illustrates how skipping foundational steps leads to significantly higher overall expenditures.

The allure of AI innovation is strong, but the path to realizing its benefits is paved with readiness. The hidden costs of poor AI integration—from data rework and employee burnout to significant opportunity losses—can quickly overshadow any perceived advantages. Readiness isn't an additional expense. It's a critical risk mitigation strategy that safeguards investment and ensures a measurable return on AI initiatives. By prioritizing foundation before innovation, organizations can confidently embark on their AI journey, transforming potential liabilities into powerful competitive advantages.]]></description>
      <content:encoded><![CDATA[Imagine pouring half a million dollars into cutting-edge AI tools, only to see zero improvement in efficiency. Worse, productivity actually drops. This isn't a hypothetical disaster; it's a common reality for many organizations. The problem isn't the AI itself. It's a critical gap in strategy: a lack of readiness.

Most companies budget for the obvious costs of AI adoption, but it's the hidden expenditures that truly erode ROI and sabotage transformation efforts. Successfully integrating AI requires much more than just buying technology. It demands a robust data foundation, strategic planning, and a prepared workforce.

Organizations typically account for direct expenses: software licenses, implementation fees, employee training. These look manageable on a balance sheet. But they're just the tip of the iceberg, a fraction of the true investment needed for AI success.

The real financial drain and strategic setback come from the invisible costs of unreadiness. These insidious factors turn promising AI projects into costly failures, impacting budgets, employee morale, and long-term innovation.

First, rework due to poor data quality. AI models are only as good as the data they're trained on. If data is inconsistent, incomplete, or inaccurate, AI outputs will require extensive manual fixing and validation. That immediately negates any efficiency gains AI promises. This often forces costly data readiness assessments just to clean and structure data effectively, *before* an organization can even launch an AI pilot.

Next, employee resistance and shadow systems. When AI tools are introduced without adequate preparation, training, or a clear perceived benefit, employees often resist. This resistance manifests as workarounds, wasted time, and the creation of shadow IT systems—unofficial tools and processes that undermine centralized AI efforts and create new security and data governance challenges.

Paradoxically, AI can sometimes increase, not decrease, employee workload and lead to burnout if not implemented thoughtfully. An Upwork study found that seventy-seven percent of employees felt AI tools actually *increased* their workload. Seventy-one percent reported burnout. When AI initiatives add to employee burden without providing clear benefits or offloading tedious tasks, it leads to frustration, disengagement, and increased employee turnover. That's a substantial loss of institutional knowledge and productivity.

Then there are significant opportunity costs. Failed AI projects don't just waste resources; they delay genuine transformation by eighteen to twenty-four months or more. This means competitors who successfully leverage AI pull ahead in efficiency, innovation, and market share. Many seemingly high-value AI use cases, like productivity improvements, can actually result in a negative ROI when deployed at scale, if an organization doesn't plan out the platform, data readiness, and governance costs.

The cumulative effect of these hidden costs is staggering. A poorly planned or executed AI implementation can end up costing an organization three to four times its original investment. This includes not just the initial outlay, but also the expenses of retooling, re-training, fixing data, managing resistance, and ultimately restarting initiatives from scratch. The real-world consequences of rushing AI integration without thorough planning are evident in these magnified costs.

Achieving a positive return on AI investment hinges on establishing a solid foundation. AI readiness isn't an optional extra; it's a prerequisite for success. The philosophy is clear: foundation before innovation. This means ensuring data is clean, accessible, and governed. Infrastructure must be capable, and the team equipped with the right skills and mindset. Without this readiness, AI becomes a liability, not an asset.

Avoiding these pitfalls requires a systematic and strategic approach. First, conduct comprehensive AI readiness assessments *before* investment. Understand current capabilities, data quality, and organizational culture before committing significant funds to tools. This involves evaluating, aligning, and accelerating AI programs with structured assessments and tailored workshops.

Second, pilot before scaling. Start with small, controlled pilot projects to test AI solutions, validate effectiveness, and identify potential challenges in a low-risk environment. This iterative approach allows for learning and adjustment.

Third, training before rollout. Invest in robust training programs that not only teach employees how to use AI tools, but also explain the 'why' behind the technology. Foster a culture of augmented intelligence where AI supports human capabilities, rather than replacing them.

Consider a non-profit organization that invested one hundred thousand dollars in AI tools without first addressing its chaotic data landscape. The organization quickly found its AI outputs unusable. It was forced to spend an additional two hundred thousand dollars on data cleanup and governance *before* seeing any value. This clearly illustrates how skipping foundational steps leads to significantly higher overall expenditures.

The allure of AI innovation is strong, but the path to realizing its benefits is paved with readiness. The hidden costs of poor AI integration—from data rework and employee burnout to significant opportunity losses—can quickly overshadow any perceived advantages. Readiness isn't an additional expense. It's a critical risk mitigation strategy that safeguards investment and ensures a measurable return on AI initiatives. By prioritizing foundation before innovation, organizations can confidently embark on their AI journey, transforming potential liabilities into powerful competitive advantages.]]></content:encoded>
      <guid isPermaLink="false">ThehiddencostsofAIunreadinessandhowtoavoidthem</guid>
      <pubDate>Thu, 04 Dec 2025 20:13:10 GMT</pubDate>
      <enclosure
        url="https://oimqzyfmglyhljjuboek.supabase.co/storage/v1/object/public/AIDM_Podcast_Audio_Files/ThehiddencostsofAIunreadinessandhowtoavoidthem.mp3"
        length="12345678"
        type="audio/mpeg" />
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:explicit>false</itunes:explicit>
    </item>

  
    <item>
      <title><![CDATA[From resistance to results: The 60-day change management timeline]]></title>
      <description><![CDATA[Successful AI implementation often has less to do with the technology itself, and more to do with people. Organizations frequently encounter resistance to new tools and processes, and this can stall innovation and delay your return on investment. But change doesn't have to be a slow, arduous process.

There's a strategic 60-day change management timeline designed to accelerate AI adoption, transforming initial reluctance into tangible results. This 60-day timeframe offers a critical balance: it's long enough to embed meaningful changes and foster new habits, yet short enough to maintain high momentum and prevent change fatigue. This systematic approach ensures that AI initiatives are not just launched, but successfully integrated, reinforcing the principle of foundation before innovation that underpins sustainable growth.

The initial two weeks, Days 1 through 14, are crucial for laying the foundation and strategic communication. Start by identifying and empowering internal champions. These are individuals enthusiastic about AI and respected by their peers. They will be vital conduits for communication and adoption.

Simultaneously, launch a clear, compelling announcement. This communication must articulate the "why" behind the AI implementation. Focus on business benefits, improved workflows, and how AI augments employee capabilities. Addressing concerns and creating space for good-faith questions is paramount for countering negative narratives and fostering readiness. Begin targeted training for your champions, equipping them with both technical proficiency and the soft skills needed to guide their colleagues.

Days 15 through 30 focus on pilot programs and securing early wins. With champions trained, this phase empowers them to deploy the new AI tools within small, volunteer-based teams. This pilot approach allows for immediate feedback and iterative refinement without widespread disruption.

Critically, document specific time savings, efficiency gains, and problem-solving successes. Collecting testimonials from these early adopters provides powerful social proof and validates the investment. These early wins are essential for building confidence and demonstrating tangible benefits, laying the foundation for broader acceptance.

Days 31 through 45 involve a broader rollout and peer-led learning. Building on the success of the pilot, this period involves a phased expansion. Conduct department-by-department training sessions, with champions playing a leading role. Peer-led training often resonates more effectively than top-down instruction. Colleagues can share practical tips and address context-specific concerns.

Foster an environment of continuous learning by organizing weekly success story sharing sessions. These forums allow teams to highlight how AI is enhancing their work, providing inspiration and practical examples for others. This visible recognition of progress fuels momentum and reinforces the value proposition of the new tools.

The final two weeks, Days 46 through 60, are dedicated to optimization, scaling, and measuring ROI. Based on accumulated feedback, optimize the AI tools and accompanying processes to maximize user experience and effectiveness. Expand the deployment to remaining teams, leveraging the established successes and the champion network.

Crucially, begin measuring key ROI metrics. While many companies see ROI in two to four years, top performers can achieve returns in under twelve months, with leading adopters seeing 3.7 times ROI per dollar spent. This systematic 60-day approach helps accelerate that timeline, transforming initial resistance into measurable business value.

Several critical factors contribute to rapid AI adoption. First, leadership visibility. Executives must actively champion the change, communicating its strategic importance and supporting employees through the transition. Second, champion empowerment. Equip internal advocates with the necessary tools, training, and authority to lead their peers. Third, celebrate wins. Publicly recognize and celebrate incremental successes to build morale and reinforce positive behaviors. Finally, foster psychological safety. Cultivate an environment where employees feel safe to ask questions, voice concerns, and experiment without fear of reprisal.

Consider a healthcare organization that, by adopting a similar structured timeline, boosted AI adoption from 30% to over 90% within 60 days. By systematically addressing resistance, fostering leadership, and empowering internal advocates, they achieved rapid, transformative results.

The notion that organizational change is inherently slow is a myth. When approached systematically, with clear objectives and a structured timeline, significant transformation can occur far more quickly than anticipated. This 60-day playbook offers a robust framework for accelerating AI adoption, ensuring that an organization builds a strong foundation before innovation and realizes tangible results sooner.]]></description>
      <content:encoded><![CDATA[Successful AI implementation often has less to do with the technology itself, and more to do with people. Organizations frequently encounter resistance to new tools and processes, and this can stall innovation and delay your return on investment. But change doesn't have to be a slow, arduous process.

There's a strategic 60-day change management timeline designed to accelerate AI adoption, transforming initial reluctance into tangible results. This 60-day timeframe offers a critical balance: it's long enough to embed meaningful changes and foster new habits, yet short enough to maintain high momentum and prevent change fatigue. This systematic approach ensures that AI initiatives are not just launched, but successfully integrated, reinforcing the principle of foundation before innovation that underpins sustainable growth.

The initial two weeks, Days 1 through 14, are crucial for laying the foundation and strategic communication. Start by identifying and empowering internal champions. These are individuals enthusiastic about AI and respected by their peers. They will be vital conduits for communication and adoption.

Simultaneously, launch a clear, compelling announcement. This communication must articulate the "why" behind the AI implementation. Focus on business benefits, improved workflows, and how AI augments employee capabilities. Addressing concerns and creating space for good-faith questions is paramount for countering negative narratives and fostering readiness. Begin targeted training for your champions, equipping them with both technical proficiency and the soft skills needed to guide their colleagues.

Days 15 through 30 focus on pilot programs and securing early wins. With champions trained, this phase empowers them to deploy the new AI tools within small, volunteer-based teams. This pilot approach allows for immediate feedback and iterative refinement without widespread disruption.

Critically, document specific time savings, efficiency gains, and problem-solving successes. Collecting testimonials from these early adopters provides powerful social proof and validates the investment. These early wins are essential for building confidence and demonstrating tangible benefits, laying the foundation for broader acceptance.

Days 31 through 45 involve a broader rollout and peer-led learning. Building on the success of the pilot, this period involves a phased expansion. Conduct department-by-department training sessions, with champions playing a leading role. Peer-led training often resonates more effectively than top-down instruction. Colleagues can share practical tips and address context-specific concerns.

Foster an environment of continuous learning by organizing weekly success story sharing sessions. These forums allow teams to highlight how AI is enhancing their work, providing inspiration and practical examples for others. This visible recognition of progress fuels momentum and reinforces the value proposition of the new tools.

The final two weeks, Days 46 through 60, are dedicated to optimization, scaling, and measuring ROI. Based on accumulated feedback, optimize the AI tools and accompanying processes to maximize user experience and effectiveness. Expand the deployment to remaining teams, leveraging the established successes and the champion network.

Crucially, begin measuring key ROI metrics. While many companies see ROI in two to four years, top performers can achieve returns in under twelve months, with leading adopters seeing 3.7 times ROI per dollar spent. This systematic 60-day approach helps accelerate that timeline, transforming initial resistance into measurable business value.

Several critical factors contribute to rapid AI adoption. First, leadership visibility. Executives must actively champion the change, communicating its strategic importance and supporting employees through the transition. Second, champion empowerment. Equip internal advocates with the necessary tools, training, and authority to lead their peers. Third, celebrate wins. Publicly recognize and celebrate incremental successes to build morale and reinforce positive behaviors. Finally, foster psychological safety. Cultivate an environment where employees feel safe to ask questions, voice concerns, and experiment without fear of reprisal.

Consider a healthcare organization that, by adopting a similar structured timeline, boosted AI adoption from 30% to over 90% within 60 days. By systematically addressing resistance, fostering leadership, and empowering internal advocates, they achieved rapid, transformative results.

The notion that organizational change is inherently slow is a myth. When approached systematically, with clear objectives and a structured timeline, significant transformation can occur far more quickly than anticipated. This 60-day playbook offers a robust framework for accelerating AI adoption, ensuring that an organization builds a strong foundation before innovation and realizes tangible results sooner.]]></content:encoded>
      <guid isPermaLink="false">FromresistancetoresultsThe60daychangemanagementtimeline</guid>
      <pubDate>Fri, 05 Dec 2025 19:38:33 GMT</pubDate>
      <enclosure
        url="https://oimqzyfmglyhljjuboek.supabase.co/storage/v1/object/public/AIDM_Podcast_Audio_Files/FromresistancetoresultsThe60daychangemanagementtimeline.mp3"
        length="12345678"
        type="audio/mpeg" />
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:explicit>false</itunes:explicit>
    </item>


  
    <item>
      <title><![CDATA[Your first AI project: A 30-day pilot framework]]></title>
      <description><![CDATA[Many organizations eager to harness artificial intelligence often stumble at the first hurdle: selecting the right inaugural project. The common pitfalls include choosing initiatives that are either too grand and complex, leading to prolonged development and stalled progress, or too trivial, failing to demonstrate meaningful business value. This often results in AI efforts losing momentum before they can truly begin.

Artificial intelligence implementation requires digital infrastructure. Clean data, connected systems, unified platforms—without this foundation, automation efforts are likely to fail. This is why a strategic pilot is crucial. It builds confidence and proves tangible value quickly. A practical thirty-day framework guides executives and data leaders in launching their first AI project successfully, transforming initial skepticism into actionable insights and measurable returns.

By focusing on achievable outcomes within a concise timeframe, this framework helps organizations build foundational AI capabilities, gain vital experience, and secure executive buy-in for future, larger-scale transformations.

The key to a successful first AI project lies in finding the just-right balance. It needs to be impactful enough to matter but contained enough to execute quickly. Here are the Goldilocks criteria for your first AI pilot:

First, the task must be repetitive enough to show clear time savings. Look for tasks that consume significant manual effort and occur frequently. Automation in these areas provides an immediate, quantifiable benefit in terms of efficiency.

Second, it must be important enough that people care about improvement. The project should address a genuine pain point for a specific team or department, fostering engagement and adoption.

Third, it needs to be simple enough to implement in thirty days. Avoid projects requiring extensive data integration, complex model training, or significant infrastructure changes. The goal is rapid deployment and feedback.

Finally, it must be safe enough that errors won't cause major problems. Choose areas where potential AI mistakes have minimal business impact. This allows for refinement without critical repercussions. Top performers often leverage AI in unglamorous but high-volume areas that no one wants to do manually.

The initial phase, days one to seven, is critical for identifying and scoping the right opportunity. Start by identifying three to five candidate processes. Brainstorm within departments, focusing on manual, repetitive tasks that fit the Goldilocks criteria. Consider processes that involve document processing, data entry, or routine communications.

Next, score these candidates on frequency, pain level, and data availability. How often does this task occur? How much does it frustrate employees or cause bottlenecks? Is the necessary data clean, accessible, and in a usable format for AI?

Choose the winner and assign a dedicated project owner from the business side, not just IT. This ensures alignment with operational needs and fosters adoption.

With a clear target, days eight to fourteen focus on quickly building a functional prototype and gathering initial feedback. The emphasis is on speed and iteration, not perfection. Create your first automation or custom GPT. Leverage readily available tools and platforms. For text-based tasks, a low-code automation platform or a custom GPT might suffice. For data processing, simple scripting or cloud-based AI services can be effective.

Test with a pilot group of three to five users. Engage the project owner and a small, representative group of end-users. Conduct hands-on testing in a controlled environment to observe real-world interaction. Then, refine based on feedback. Use the gathered input to make quick adjustments. This iterative process is key to ensuring the solution truly meets user needs and addresses initial pain points.

Once the prototype is validated, it's time to expand the pilot to a broader audience within the target department, showcasing its immediate benefits during days fifteen to twenty-three. Roll out the refined AI tool to the full department, typically ten to twenty people. Provide clear instructions and support, ensuring a smooth transition. Track the time saved per person per week. Establish simple metrics to quantify the impact. This might involve self-reporting or manager observations. Document any unexpected benefits, such as reduced errors, increased employee satisfaction, or faster response times, which can add significant value to your success story.

The final week, days twenty-four to thirty, is dedicated to quantifying the pilot's success and preparing to share the results with leadership. This builds a case for further AI investment. Quantify the time saved and understand the increased capacity it brings. For instance, if a team of ten saves two hours each week, that translates to twenty hours of productivity regained, which can be redirected to higher-value activities. Organizations that successfully implement these pilots often report significant efficiency gains and strategic advantages.

Collect user testimonials. Gather direct quotes from employees about how the AI tool has positively impacted their work. These provide powerful qualitative evidence. Finally, present results to leadership. Prepare a concise presentation highlighting the problem, the solution, the quantifiable impact, and the qualitative benefits. Focus on the foundation before innovation principle and the potential for broader impact.

To spark ideas, consider these examples. In HR, you could automate initial resume screening for common keywords, saving recruiters hours per week. For Finance, use AI to generate routine weekly or monthly financial reports from structured data, ensuring consistency and accuracy. In Operations, implement an AI tool to summarize daily status updates from project management systems into a concise executive brief.

For example, in the energy sector, a pilot project focused on automating the initial draft of proposal generation, drawing from existing templates and past project data. This specific intervention saved the proposal team an estimated twelve hours per week, allowing them to focus on customization and strategic content.

Launching your first AI project doesn't have to be a daunting, multi-year endeavor. By adopting a disciplined thirty-day pilot framework, organizations can quickly demonstrate tangible value, build internal capabilities, and cultivate a culture of innovation. Success in a small, contained project creates invaluable momentum, proving that foundational AI efforts can lead to significant, measurable transformation across the enterprise. This "start small, scale smart" approach is the bedrock of sustainable AI adoption.]]></description>
      <content:encoded><![CDATA[Many organizations eager to harness artificial intelligence often stumble at the first hurdle: selecting the right inaugural project. The common pitfalls include choosing initiatives that are either too grand and complex, leading to prolonged development and stalled progress, or too trivial, failing to demonstrate meaningful business value. This often results in AI efforts losing momentum before they can truly begin.

Artificial intelligence implementation requires digital infrastructure. Clean data, connected systems, unified platforms—without this foundation, automation efforts are likely to fail. This is why a strategic pilot is crucial. It builds confidence and proves tangible value quickly. A practical thirty-day framework guides executives and data leaders in launching their first AI project successfully, transforming initial skepticism into actionable insights and measurable returns.

By focusing on achievable outcomes within a concise timeframe, this framework helps organizations build foundational AI capabilities, gain vital experience, and secure executive buy-in for future, larger-scale transformations.

The key to a successful first AI project lies in finding the just-right balance. It needs to be impactful enough to matter but contained enough to execute quickly. Here are the Goldilocks criteria for your first AI pilot:

First, the task must be repetitive enough to show clear time savings. Look for tasks that consume significant manual effort and occur frequently. Automation in these areas provides an immediate, quantifiable benefit in terms of efficiency.

Second, it must be important enough that people care about improvement. The project should address a genuine pain point for a specific team or department, fostering engagement and adoption.

Third, it needs to be simple enough to implement in thirty days. Avoid projects requiring extensive data integration, complex model training, or significant infrastructure changes. The goal is rapid deployment and feedback.

Finally, it must be safe enough that errors won't cause major problems. Choose areas where potential AI mistakes have minimal business impact. This allows for refinement without critical repercussions. Top performers often leverage AI in unglamorous but high-volume areas that no one wants to do manually.

The initial phase, days one to seven, is critical for identifying and scoping the right opportunity. Start by identifying three to five candidate processes. Brainstorm within departments, focusing on manual, repetitive tasks that fit the Goldilocks criteria. Consider processes that involve document processing, data entry, or routine communications.

Next, score these candidates on frequency, pain level, and data availability. How often does this task occur? How much does it frustrate employees or cause bottlenecks? Is the necessary data clean, accessible, and in a usable format for AI?

Choose the winner and assign a dedicated project owner from the business side, not just IT. This ensures alignment with operational needs and fosters adoption.

With a clear target, days eight to fourteen focus on quickly building a functional prototype and gathering initial feedback. The emphasis is on speed and iteration, not perfection. Create your first automation or custom GPT. Leverage readily available tools and platforms. For text-based tasks, a low-code automation platform or a custom GPT might suffice. For data processing, simple scripting or cloud-based AI services can be effective.

Test with a pilot group of three to five users. Engage the project owner and a small, representative group of end-users. Conduct hands-on testing in a controlled environment to observe real-world interaction. Then, refine based on feedback. Use the gathered input to make quick adjustments. This iterative process is key to ensuring the solution truly meets user needs and addresses initial pain points.

Once the prototype is validated, it's time to expand the pilot to a broader audience within the target department, showcasing its immediate benefits during days fifteen to twenty-three. Roll out the refined AI tool to the full department, typically ten to twenty people. Provide clear instructions and support, ensuring a smooth transition. Track the time saved per person per week. Establish simple metrics to quantify the impact. This might involve self-reporting or manager observations. Document any unexpected benefits, such as reduced errors, increased employee satisfaction, or faster response times, which can add significant value to your success story.

The final week, days twenty-four to thirty, is dedicated to quantifying the pilot's success and preparing to share the results with leadership. This builds a case for further AI investment. Quantify the time saved and understand the increased capacity it brings. For instance, if a team of ten saves two hours each week, that translates to twenty hours of productivity regained, which can be redirected to higher-value activities. Organizations that successfully implement these pilots often report significant efficiency gains and strategic advantages.

Collect user testimonials. Gather direct quotes from employees about how the AI tool has positively impacted their work. These provide powerful qualitative evidence. Finally, present results to leadership. Prepare a concise presentation highlighting the problem, the solution, the quantifiable impact, and the qualitative benefits. Focus on the foundation before innovation principle and the potential for broader impact.

To spark ideas, consider these examples. In HR, you could automate initial resume screening for common keywords, saving recruiters hours per week. For Finance, use AI to generate routine weekly or monthly financial reports from structured data, ensuring consistency and accuracy. In Operations, implement an AI tool to summarize daily status updates from project management systems into a concise executive brief.

For example, in the energy sector, a pilot project focused on automating the initial draft of proposal generation, drawing from existing templates and past project data. This specific intervention saved the proposal team an estimated twelve hours per week, allowing them to focus on customization and strategic content.

Launching your first AI project doesn't have to be a daunting, multi-year endeavor. By adopting a disciplined thirty-day pilot framework, organizations can quickly demonstrate tangible value, build internal capabilities, and cultivate a culture of innovation. Success in a small, contained project creates invaluable momentum, proving that foundational AI efforts can lead to significant, measurable transformation across the enterprise. This "start small, scale smart" approach is the bedrock of sustainable AI adoption.]]></content:encoded>
      <guid isPermaLink="false">YourfirstAIprojectA30daypilotframework</guid>
      <pubDate>Wed, 10 Dec 2025 20:17:08 GMT</pubDate>
      <enclosure
        url="https://oimqzyfmglyhljjuboek.supabase.co/storage/v1/object/public/AIDM_Podcast_Audio_Files/YourfirstAIprojectA30daypilotframework.mp3"
        length="12345678"
        type="audio/mpeg" />
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:explicit>false</itunes:explicit>
    </item>

  
    <item>
      <title><![CDATA[AI vendor selection: The 12-question framework that prevents buyer's remorse]]></title>
      <itunes:image href="https://oimqzyfmglyhljjuboek.supabase.co/storage/v1/object/public/$input.first().json.Key" />
      <description><![CDATA[Most AI projects fail. Not because the technology isn't there, but because companies choose the wrong tools. Organizations spend an average of six months evaluating over a dozen AI vendors, only to select a solution that falls short. This leads to costly buyer's remorse and stalls real innovation.

The problem isn't a lack of effort. It's a misguided focus on shiny features, ignoring the critical implications of implementation, maintenance, and long-term strategic alignment. While ninety-one percent of organizations plan to increase AI investment, a staggering seventy-five percent of AI initiatives reportedly don't meet expectations.

To cut through the hype and prevent these failures, executives and data leaders need a robust framework. This isn't about a simple checklist; it's about asking twelve critical questions that reveal red flags and ensure your AI investments build a strong foundation.

When you're evaluating AI solutions, the conversation has to go beyond a bulleted list of features. A comprehensive assessment means diving deep into the operational, ethical, and strategic implications of partnering with any AI vendor.

Here are the twelve critical questions:

First, **data stewardship**. Where exactly does your data go? Can you export it at any time? And who owns the content generated by the AI? You need clear answers on where your data resides, how it's used for model training, and your rights to retrieve it. Crucially, clarify ownership of any content or insights the AI produces. This ensures your commercial usage rights are protected and your AI training is compliant.

Second, **integration and APIs**. Does this AI solution work seamlessly with your existing systems? Are robust APIs available for custom integrations? Without pre-built connectors or strong APIs, your new AI tool risks becoming an isolated island, creating more data silos.

Third, **training and support**. What's included in the initial package? Is ongoing support provided? Implementation requires more than just installation. Ask about training programs, documentation, and the availability of continuous support.

Fourth, **compliance and ethics**. Is the solution HIPAA, GDPR, or CCPA ready? Where are their servers physically located? Compliance with industry regulations is non-negotiable. Demand detailed information on their data governance model and how they ensure ethical AI use. Server location directly impacts data sovereignty.

Fifth, **customization**. Can you adapt this tool to your unique workflows, or must your team adapt to the tool? The ability to tailor the solution to your business processes ensures higher adoption and relevance.

Sixth, **pricing transparency**. Are there any hidden costs beyond the initial subscription? What happens when you scale your usage? Unexpected costs for data storage, API calls, or additional users can quickly derail your ROI projections.

Seventh, **performance and accuracy**. What are the typical response times under load? How is accuracy measured, and what are the benchmarks? Demand objective performance metrics to assess the tool's real-world efficacy, especially during peak conditions.

Eighth, **support and SLAs**. What's the guaranteed response time for issues? Will you have a dedicated account or support representative? Prompt and effective support is crucial for mission-critical AI tools.

Ninth, **product roadmap**. What are the vendor's future development plans? How do they incorporate user input into their roadmap? An AI solution should evolve with your business, indicating a commitment to a long-term partnership.

Tenth, **exit strategy**. Can you easily leave if the solution doesn't work out? Inquire about data portability, contract termination clauses, and any associated costs. An easy exit strategy mitigates risk and prevents you from being locked into an underperforming solution.

Eleventh, **references**. Can you speak to at least three companies similar to yours who are using this solution? The most telling insights often come directly from existing customers.

Twelfth, and finally, **real-world trial**. Can you conduct a pilot program using your actual data and workflows, not just sanitized demo data? A true test lets you evaluate the tool's effectiveness in your unique context before making a significant investment.

During your evaluation, watch for critical red flags: the absence of a trial period, vague pricing, a reluctance to provide references, or a "trust us" stance on data location and security. These are often indicators of underlying problems.

It's also crucial to consider the "build versus buy" equation. Many organizations default to buying commercial AI solutions, assuming custom development is always more expensive or complex. However, with accessible large language models and custom GPTs, internal development can deliver tailored solutions at a fraction of the cost and time.

For example, one construction company spent months evaluating eight commercial proposal generation tools. They were consistently disappointed by the lack of customization and high recurring costs. By shifting strategy, they built a custom GPT for proposal generation in just two weeks, integrating their proprietary data and branding. This saved hundreds of thousands of dollars, allowed them to own the intellectual property, and maintained full control over their data and processes.

Asking the right questions upfront can save not just six months of evaluation time, but also hundreds of thousands of dollars in misdirected investment. It's about ensuring your AI strategy truly aligns with your unique business needs and builds a solid foundation for future innovation.]]></description>
      <content:encoded><![CDATA[Most AI projects fail. Not because the technology isn't there, but because companies choose the wrong tools. Organizations spend an average of six months evaluating over a dozen AI vendors, only to select a solution that falls short. This leads to costly buyer's remorse and stalls real innovation.

The problem isn't a lack of effort. It's a misguided focus on shiny features, ignoring the critical implications of implementation, maintenance, and long-term strategic alignment. While ninety-one percent of organizations plan to increase AI investment, a staggering seventy-five percent of AI initiatives reportedly don't meet expectations.

To cut through the hype and prevent these failures, executives and data leaders need a robust framework. This isn't about a simple checklist; it's about asking twelve critical questions that reveal red flags and ensure your AI investments build a strong foundation.

When you're evaluating AI solutions, the conversation has to go beyond a bulleted list of features. A comprehensive assessment means diving deep into the operational, ethical, and strategic implications of partnering with any AI vendor.

Here are the twelve critical questions:

First, **data stewardship**. Where exactly does your data go? Can you export it at any time? And who owns the content generated by the AI? You need clear answers on where your data resides, how it's used for model training, and your rights to retrieve it. Crucially, clarify ownership of any content or insights the AI produces. This ensures your commercial usage rights are protected and your AI training is compliant.

Second, **integration and APIs**. Does this AI solution work seamlessly with your existing systems? Are robust APIs available for custom integrations? Without pre-built connectors or strong APIs, your new AI tool risks becoming an isolated island, creating more data silos.

Third, **training and support**. What's included in the initial package? Is ongoing support provided? Implementation requires more than just installation. Ask about training programs, documentation, and the availability of continuous support.

Fourth, **compliance and ethics**. Is the solution HIPAA, GDPR, or CCPA ready? Where are their servers physically located? Compliance with industry regulations is non-negotiable. Demand detailed information on their data governance model and how they ensure ethical AI use. Server location directly impacts data sovereignty.

Fifth, **customization**. Can you adapt this tool to your unique workflows, or must your team adapt to the tool? The ability to tailor the solution to your business processes ensures higher adoption and relevance.

Sixth, **pricing transparency**. Are there any hidden costs beyond the initial subscription? What happens when you scale your usage? Unexpected costs for data storage, API calls, or additional users can quickly derail your ROI projections.

Seventh, **performance and accuracy**. What are the typical response times under load? How is accuracy measured, and what are the benchmarks? Demand objective performance metrics to assess the tool's real-world efficacy, especially during peak conditions.

Eighth, **support and SLAs**. What's the guaranteed response time for issues? Will you have a dedicated account or support representative? Prompt and effective support is crucial for mission-critical AI tools.

Ninth, **product roadmap**. What are the vendor's future development plans? How do they incorporate user input into their roadmap? An AI solution should evolve with your business, indicating a commitment to a long-term partnership.

Tenth, **exit strategy**. Can you easily leave if the solution doesn't work out? Inquire about data portability, contract termination clauses, and any associated costs. An easy exit strategy mitigates risk and prevents you from being locked into an underperforming solution.

Eleventh, **references**. Can you speak to at least three companies similar to yours who are using this solution? The most telling insights often come directly from existing customers.

Twelfth, and finally, **real-world trial**. Can you conduct a pilot program using your actual data and workflows, not just sanitized demo data? A true test lets you evaluate the tool's effectiveness in your unique context before making a significant investment.

During your evaluation, watch for critical red flags: the absence of a trial period, vague pricing, a reluctance to provide references, or a "trust us" stance on data location and security. These are often indicators of underlying problems.

It's also crucial to consider the "build versus buy" equation. Many organizations default to buying commercial AI solutions, assuming custom development is always more expensive or complex. However, with accessible large language models and custom GPTs, internal development can deliver tailored solutions at a fraction of the cost and time.

For example, one construction company spent months evaluating eight commercial proposal generation tools. They were consistently disappointed by the lack of customization and high recurring costs. By shifting strategy, they built a custom GPT for proposal generation in just two weeks, integrating their proprietary data and branding. This saved hundreds of thousands of dollars, allowed them to own the intellectual property, and maintained full control over their data and processes.

Asking the right questions upfront can save not just six months of evaluation time, but also hundreds of thousands of dollars in misdirected investment. It's about ensuring your AI strategy truly aligns with your unique business needs and builds a solid foundation for future innovation.]]></content:encoded>
      <guid isPermaLink="false">AIvendorselectionThe12questionframeworkthatpreventsbuyersremorse</guid>
      <pubDate>Mon, 15 Dec 2025 16:34:20 GMT</pubDate>
      <enclosure
        url="https://oimqzyfmglyhljjuboek.supabase.co/storage/v1/object/public/AIDM_Podcast_Audio_Files/AIvendorselectionThe12questionframeworkthatpreventsbuyersremorse_merged.mp3"
        length="12345678"
        type="audio/mpeg" />
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:explicit>false</itunes:explicit>
    </item>

  </channel>
</rss>